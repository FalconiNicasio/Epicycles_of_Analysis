---
title: "Final Project"
author: "Falconi Nicasio"
date: "April 2, 2019"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Summary

This is an exploration and analysis of the data set `bikeVendors` provided in **i590 Applied Data Science Spring 2019** at *Indiana University*. It follows very closely to the readings of [*The Art of Data Science: A Guide for Anyone Who Works with Data*](https://leanpub.com/artofdatascience) by Roger D. Peng and Elizabeth Matsui. 



Before we begin it should be known that this project will follow the structure of the *Epicycle of Analysis* noted in *The Art of Data Science (Peng and Matsui 2008)* which is listed below.

## Epicycle of Analysis

1. Stating and refining the question
2. Exploring the data
3. Building formal statistical models
4. Interpreting the results
5. Communicating the results

For each of these steps you must go through

* Setting expectations
* Collecting information (data), comparing the data to your expectations, and if the expectations don't match,
* Revising your expectations or fixing the data so your data and your expectations match.

```{r data}
#load data
df <- read.csv(file = "../data/bikeVendors.csv")
```

## Step 1: Stating and refining the question

Let's say your employer wants to make a new model bicycle and wants to know what kind of bicycle to put into production.

You're given this dataset to work with but before you look at it, you want to start by thinking about what kind of questions you want to answer.

>"How many people don't have bicycles?"

>"What are the predictors?"

>"Are these good questions?"

According to *Chapter 3. Stating and Refining Questions from Peng and Matsui (2018)*, there are 6 **types of questions**:

1. Descriptive
    - summarizes the characteristic of the data
2. Exploratory
    - looking for patterns, trends, or relationsips between variables
3. Inferential
    - Infer from a different data set
4. Predictive
    - Find predictive features
5. Causal
    - What causes the effect
6. Mechanistic
    - How it causes it
    
Furthermore, you know the **characteristics of a good question** (*Peng and Matsui 2018*):

1. Should be of **interest** to audience
2. Not already answered
3. Plausible
4. Answerable
5. Specific
    
After considering these things you realize that a quick search on "How many people don't have bicycles" can easily be answered.

>"About 15 percent of Americans - or 45 million people - made at least one bicycle trip for transportation in the last year.Mar 4, 2015" - usa.streetsblog.org

With new knowledge you then refine your question to be more precise.

>"What type of bicycles are people most interested in?"

Next, you note that your questions are *Exploratory and Predictive* which will drive your approach as you take a look at the data. You also start to develop expectations by following the 3 subset of steps learned from the *Epicycle of Analysis*. From personal experience, you might expect there to be at least two types of bike riders: One as a road cyclist, and a second as a mountain cyclist. Also, if we were to think about what we would consider when shopping for a bicycle, one might think that price and budget would play a huge factor in your decision. Let's now take a look at the data.

```{r}
head(df)
```

Here, we can see that most of the data are **sale percentages** of different models of bicycles across bike vendors. We also notice the data set contains `category1` as main categories of `Road` or `Mountain` bikes and subcategories listed as `category2`. Lastly, we see that there is a `frame` column and a `price` column. After collecting the data and comparing it to our expectations, we see that they match but we realized that the question, **"What are the predictors?"**, will automatically be answered as we explore and analyze the data. In this project it should be noted that we will only work with the features presented in this data but in other projects you may have to pull data from other sources in order to find more predictors and features. Since we have a relationship between different bike vendors and different types of bikes, we can update the question with,

>"Where would our bicycle sell the best?"


## Step 2: Explore the data

It's always good to do a quick descriptive analysis of the data before you begin modeling.

The `str()` function lays out some important descriptives. At the top it shows 97 observations (rows) and 35 variables/features (columns). Then it shows the type of each variable: `model`, `category1`, `category2`, `frame` are all `Factor` or categorical variables, and the rest of the 35 variables are all etiher `int` (discrete) or `num` (continuous) variables.

```{r descriptives}
str(df)
```

Next, we should check for missing values.

```{r}
sum(is.na(df))
```

In this case there are no missing values.


Lets continue our Exploratory Data Analysis (EDA) by using the `summary()` function which provides statistical information like `mean`, `median`, `quantiles`, `min`, and `max`. But before that, we should again set our expectations to get a "ball park" image in our head before we dive into the data.

An example of expectations would be:
* `category1` might have more road bikes than mountain bikes.
* `price` range will be between 200 and 10,000.
* A few `model` bikes will have a large share of sales.

```{r summary}
summary(df)
```

Comparing our expectations and the data we can see that there is a close to even split of `Mountain` (51) and `Road` bikes (46). The `price` range is from 415 to 12,790 with the average being 3,954. And looking at the `max` sale perentage of any of the vendors is around .05 with the `mean` of any particular bicycle sale percentage to be around .01. What this tells us is that there aren't any single `model` bicycle that dominates the market. Without having anything else to compare with, we just have to assume the data is correct and that we just change our expectations to match the data which overall were pretty close to the actual data.

The next step might be to see if there are any correlations between bike vendors that could help us determine if there are any patterns in our data.

```{r corrplot}
library(corrplot)

res <- cor(df[ ,-(1:5)])

corrplot(res, type = "upper", tl.col = "black", tl.cex = .5, order = "hclust")

```

Notice the `corrplot` function specified `order = "hclust"` which did a hierarchical cluster of bike vendors that are strongly correlated with each other. This can quickly show us if there are any groups we could cluster together and from the plot we can see that there are.

## Step 3: Building formal statistical models


### Cluster Analysis

As we saw in the previous step of exploring the data, we saw from the `corrplot` that there are definite clusters in the data. Commonly, this brings up a new question.

>"How many clusters are there?"

This is an important question because this can also lead to an analysis of the different types of customers in the market. Here we will be implementing a `Kmeans()` model.

The idea of clustering is so to group the data in similar categories in hopes the results will be meaningful to our question. Since we are interested in the consumer demographics of bicycles, the first step is to transpose the current data frame to have vendors as observations (rows) and bicycles as features (columns).

```{r df.t}
df.t <- as.data.frame(t(df))
colnames(df.t) <- df$model
```

Not knowing how many clusters we should have we need to run an `Elbow` and `Silhouette` plot. From exploring the data earlier, we know that there are two types of bikes, `Road` and `Mountain`. Also, each bicycle has a different `price` and we could expect consumers to shop based on budget whether they are looking for a more expensive bike or a cheaper one. With these two variables we can safely assume that there should be atleast 3 or more clusters, one for `Road`, one for `Mountain`, and one for `price`.

The first plot generated to find how many clusters we need is called an Elbow or Scree plot. This basically takes **total within-cluster sum of squares** for a range of `k` or `centers` of the `kmean()` model and plots them with `k` on the x-axis and **total within-cluster sum of squares** on the y-axis. It's called the "elbow" method because you subjectively decide at which point `k` does it start to level off creating what an "elbow".

```{r elbow}
library(tidyverse)
library(purrr)

set.seed(1234)
tot_withinss <- map_dbl(1:10, function(k){
  model <- kmeans(x = df.t[-(1:5),], centers = k)
  model$tot.withinss
})

elbow_df <- data.frame(
  k = 1:10,
  tot_withinss = tot_withinss
)

theme_update(plot.title = element_text(hjust = 0.5))

ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:10) +
  ggtitle("Elbow (Scree) Plot") +
  xlab("k (centers)") +
  ylab("Total within-cluster sum of squares")
```

This Elbow plot is interesting because there are two points we could consider the elbow, the first being at k = 4 and the second at k = 6. 

Let's use a second tool to try and identify how many clusters we need, and make a Silhouette plot.

The Silhouette plot is similar to the Elbow plot but the Silhouette plot takes the average of all silhouette widths (y-axis) and plots them against k, the number of clusters.


```{r silhouette}
library(cluster)

set.seed(1234)
sil_width <- map_dbl(2:10, function(k){
  model <- pam(x = df.t[-(1:5),], k = k)
  model$silinfo$avg.width
})

sil_df <- data.frame(
  k = 2:10,
  sil_width = sil_width
)

ggplot(sil_df, aes(x = k, y = sil_width)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 2:10) +
  ggtitle("Silhouette Plot") +
  xlab("k (centers)") +
  ylab("Average Silhouette Width")
```

Here in the plot, the highest value of average widths is actually k = 2. But going back to our expectations of having 3 or more clusters, we take the second highest value of k = 4. Along with the Elbow plot, we are going to assume that we should have 4 clusters.

Now that we know how many clusters we need,the next step is to create the clusters via **K-means Clustering**. Here we create the model on `df.t` that does not include the categorical observations (`model`, `category1`, `category2`, and `frame`). We will also leave out `price`, although it is a continuous variable, for this data set we are only concerned with the sales percentages from all the vendors of each bicycle model.


```{r cluster_dist}
set.seed(1234)
kmeans_model <- kmeans(x = df.t[-(1:5),], centers = 4)

kclusters <- kmeans_model$cluster

tmpdf <- df.t[-(1:5),]
tmpdf$cluster <- kclusters

count(tmpdf, cluster)
```

After running a `count` on the newly created clusters, we can see the distribution of clusters. It looks like most of the vendors are in cluster 2. Let's explore more to see if this is meaningful.

```{r list_clusters}
for (i in 1:4) {
  print(kclusters[kclusters == i])
}
```


```{r}
#gather cluster centroids
cluster_means <- as.data.frame(t(kmeans_model$centers))
colnames(cluster_means) <- c('C1', 'C2', 'C3', 'C4')
#create new data frame with centroids and categorial data
df.clustered <- cbind(df[,1:5], cluster_means)
```



```{r cluster1}
library(knitr)

kable(
  df.clustered %>%
  arrange(desc(C1)) %>%
  select(model, category1, category2, frame, price, C1) %>%
  top_n(10))
```

```{r cluster2}
kable(
  df.clustered %>%
  arrange(desc(C2)) %>%
  select(model, category1, category2, frame, price, C2) %>%
  top_n(10))
```

```{r cluster3}
kable(
  df.clustered %>%
  arrange(desc(C3)) %>%
  select(model, category1, category2, frame, price, C3) %>%
  top_n(10))
```

```{r cluster4}
kable(
  df.clustered %>%
  arrange(desc(C4)) %>%
  select(model, category1, category2, frame, price, C4) %>%
  top_n(10))
```


From exploring the clusters more thoroughly, we can see some patterns develop between the clusters.

**Cluster 1** has 6 vendors: Cincinnati.Speed, Columbus.Race.Equipment, Las.Vegas.Cycles, Louisville.RaceEquipment, San.Francisco.Cruisers, & Wichita.Speed. 

Their top 10 selling bikes are all `Road` bikes on the more expensive side with a range of `3200` up to `9590`. All but one were made out of `Carbon`.

**Cluster 2** has 13 vendors (the most of any cluster):  Albuquerque.Cycles, Dallas.Cycles, Denver.Bike.Shop, Detroit.Cycles, Kansas.City.29ers, Los.Angeles.Cycles, Minneapolis.Bike.Shop, New.York.Cycles, Philadelphia.Bike.Shop, Phoenix.Bi.peds, Portland.Bi.peds, Providence.Bi.peds, San.Antonio.Bike.Shop.

Their top 10 selling bikes are a mix of `Mountain` and `Road` bikes that are on the lower end of price with a range of `815` to `2880`. Only 3 bikes have a `Carbon` frame.

**Cluster 3** has 8 vendors: Ann.Arbor.Speed, Austin.Cruisers, Indianapolis.Velocipedes, Miami.Race.Equipment, Nashville.Cruisers, New.Orleans.Velocipedes, Oklahoma.City.Race.Equipment, Seattle.Race.Equipment 

Their top 10 selling bikes are all `Road` bikes that are on the lower end of price with a range of `1030` to `3200`. There is an even mix of `Carbon` and `Aluminum` frames.

**Cluster 4** has only 3 vendors: Ithaca.Mountain.Climbers,  Pittsburgh.Mountain.Machines, Tampa.29ers

Their top 10 selling bikes are all `Mountain` bikes with the exception of one `Road` bike and are mostly in the mid to upper range of price from `3200` to `6390` if we were to remove the one outlier `price` of `415`. All bicycles are made of `Carbon` with again the only outier bike made out of `Aluminum`.


Now that we have gathered all the information and described our findings the final step of cluster analysis is to determine if any of this is meaningful. Typically, which is also noted in the book that 

>"interpretation is actually happening continuously throughout an analysis" - Peng and Matou 2008

we would now interpret these results but this will be saved for the next step **Step 4: Interpreting the results**

### Regression Analysis

The next model we will use is a simple linear model `lm()`. From what we have seen in the Cluster Analysis, we have certain expectations. **Cluster 1** and **Cluster 4** have higher priced bikes while **Cluster 2** and **Cluster 3** have lower priced bicycles as their best sellers. Additionally, we might expect that for **Cluster 1** and **Cluster 4** we could see a positive regression of sales as price increases and a negative regression for **Cluster 2** and **Cluster 3**.


Let's create **Cluster** data frames with `price` included.

```{r clustersdf}
#create cluster df
cluster1 <- cbind(df[names(kclusters[kclusters == 1])], price = df$price)
cluster2 <- cbind(df[names(kclusters[kclusters == 2])], price = df$price)
cluster3 <- cbind(df[names(kclusters[kclusters == 3])], price = df$price)
cluster4 <- cbind(df[names(kclusters[kclusters == 4])], price = df$price)
```

Since we are comparing `price` to `sales` lets flatten the data frames and use `melt()`  to reshape them to have all the `sales` in one column and the `price` matched to it in a second column.

```{r}
library(reshape2)

tmp1 <- melt(cluster1, id.vars = "price", variable.name = "vendor", value.name = "sales")
tmp2 <- melt(cluster2, id.vars = "price", variable.name = "vendor", value.name = "sales")
tmp3 <- melt(cluster3, id.vars = "price", variable.name = "vendor", value.name = "sales")
tmp4 <- melt(cluster4, id.vars = "price", variable.name = "vendor", value.name = "sales")
```

#### Checking Assumptions

It's always good practice to check the assumptions of a linear model in order to confirm whether it is an appropriate model or not. 

Before we begin lets explore the data frames we created and check the distributions of `sales` through histogram plots.

```{r}
library(gridExtra)

hist1 <- ggplot(tmp1) +
  geom_histogram(aes(x = sales),bins = 50) +
  ggtitle("Sales Distribution, Cluster 1") +
  xlab("Sales (percentage)")

hist2 <- ggplot(tmp2) +
  geom_histogram(aes(x = sales),bins = 50) +
  ggtitle("Sales Distribution, Cluster 2") +
  xlab("Sales (percentage)")

hist3 <- ggplot(tmp3) +
  geom_histogram(aes(x = sales),bins = 50) +
  ggtitle("Sales Distribution, Cluster 3") +
  xlab("Sales (percentage)")

hist4 <- ggplot(tmp4) +
  geom_histogram(aes(x = sales),bins = 50) +
  ggtitle("Sales Distribution, Cluster 4") +
  xlab("Sales (percentage)")

grid.arrange(hist1, hist2, hist3, hist4, nrow = 2)
```

The distributions shows us a lot of the sales from bike vendors have bikes that yeild a zero percent sales percentage and most of the sales fall between 0 and 0.03 percent.

Now lets create our linear models and continue to check the assumptions.

```{r linear models}
# create linear models
fit_1 <- lm(sales ~ price, data = tmp1)
fit_2 <- lm(sales ~ price, data = tmp2)
fit_3 <- lm(sales ~ price, data = tmp3)
fit_4 <- lm(sales ~ price, data = tmp4)
```

The following shows **Q-Q Plots** to check for *Normality* which measures actual `qunatiles` with predicted `quantiles`. If most of the data points follow the linear line then we can assumed *Normality*.

```{r normality, fig.height = 8, fig.width=8}
par(mfrow = c(2,2))

qqnorm(resid(fit_1), main = "Normal Q-Q Plot, Cluster 1", col = "darkgrey")
qqline(resid(fit_1), col = "dodgerblue", lwd = 2)

qqnorm(resid(fit_2), main = "Normal Q-Q Plot, Cluster 2", col = "darkgrey")
qqline(resid(fit_2), col = "dodgerblue", lwd = 2)

qqnorm(resid(fit_3), main = "Normal Q-Q Plot, Cluster 3", col = "darkgrey")
qqline(resid(fit_3), col = "dodgerblue", lwd = 2)

qqnorm(resid(fit_4), main = "Normal Q-Q Plot, Cluster 4", col = "darkgrey")
qqline(resid(fit_4), col = "dodgerblue", lwd = 2)

```

These Q-Q plots are interesting. We can see that most of the data points all within *Normality* but have tapering tails. For this project we will assume *Normality*.


Next, we're going to check for *Equal Variance* via Breusch-Pagan test by using the `bptest()` function.

```{r equal variance}
library(lmtest)

bptest(fit_1)
bptest(fit_2)
bptest(fit_3)
bptest(fit_4)
```

Each linear model has a high `p-value` other than `fit_1` but can still be considered within the limits to fail to reject the null of homoscedasticity.Or in other words they all pass the *Equal Variance* test.

While this data may not be ideal, the most important part is whether the linear model makes sense. Based on our assumptions and expectations we can safely say that a linear model how price affects sales makes sense and we proceed with the model. 


#### Regression plots

An important analysis tool of regression is the visual scatter plot of the data points with the linear regression line.

```{r regression plots}
grid1 <- ggplot(tmp1, aes(x = price, y = sales)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("Regression Plot, Cluster 1") +
  xlab("Price ($)") +
  ylab("Sales (percentage")

grid2 <- ggplot(tmp2, aes(x = price, y = sales)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("Regression Plot, Cluster 2") +
  xlab("Price ($)") +
  ylab("Sales (percentage")

grid3 <- ggplot(tmp3, aes(x = price, y = sales)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("Regression Plot, Cluster 3") +
  xlab("Price ($)") +
  ylab("Sales (percentage")

grid4 <- ggplot(tmp4, aes(x = price, y = sales)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("Regression Plot, Cluster 4") +
  xlab("Price ($)") +
  ylab("Sales (percentage")
grid.arrange(grid1, grid2, grid3, grid4, nrow = 2)
```

We can see that the data match our expectations. For **Cluster 1** and **Cluster 4** we have a positive regression and for **Cluster 2** and **Cluster 3** we have a downwards negative regression. We have successfully ran two models through our data sets and we can move on to the next step.

## Step 4: Interpreting the results

Interpreting the results is possibly the most important step in the *Epicycle of Analysis*. This is where we decide whether our analysis is meaningful. Before we start let's first revisit the original questions.

>"What kind of bicycle are people most interested in?"

>"Where would our bicycle sell the best?"

The primary statistical model is the K-means cluster analysis. Here we found customer segmentation of 4 groups of bike vendors. 

**Cluster 1**  customers preferred `Road` bikes over `Mountain` and their top 10 selling bikes are high-end with `Carbon` frames and a high `price`. The regression plot showed that as price increases sales also slightly increase further confirming customers in this group prefered higher end bikes.

**Cluster 2** customers doesn't have a preference of type of bike whether it is a `Road` or `Mountain` (although further analysis will be needed to confirm as this could happen by chance). Similarily, the `frame` material seemed to not have a preference but the top 10 bicycles had a range of 815 to 2880 which showed this group preferred lower cost bikes over the more expensive ones. The regression plot showed when price increased the sales went down.

**Cluster 3** customers had similar results to Cluster 2 in the way that they preferred more affordable bikes in `price` and that `frame` material didn't matter as much. But the top 10 selling bikes were all `Road` bikes. Regression model confirmed that higher prices lowered sales.

Finally, **Cluster 4** customers preferred high-end `Mountain` bikes. All but 1 (the `Supersix Evo Hi-Mod Dura Ace 2`) were all `Mountain` bikes in their top 10. Interestingly, the 7th top selling bike is  `Catalyst 4` which is an `Aluminum` low cost (415) `Mountain` bike. This might be an error because the rest of the bikes in the top 10 have a `price` between 3200 and 6390 and all have `Carbon` frames and the regression plot showed an increase in `sales` when `price` increased.

Since *Cluster 2* has 13 vendors and *Cluster 3* has 8 vendors, totalling 21 of the 30 vendors it could be concluded that a low to mid range bike priced between 800 to 3000 is most popular. Also, the majority of top selling bikes were `Road` bikes with a mix of `Aluminum` and `Carbon` frames. Overall, these are the most popular bikes.

To answer the second question, we could just use the vendors in *Cluster 2* and *Cluster 3* to sell a `Road` bike, priced between 800 and 3000, with either type of material `frame`. This seems to be vendors for the general public while the other vendors are more specialized. This could be really helpful in saving costs when distributing production bikes to be sold.

## Step 5: Communicating the results

As the final step in the **Epicycle of Analysis** this is usually the actual report and the presentation of your findings. A part of this step is to still go through the 3 subset of steps and gather data or feedback. With that being said, it is important to get feedback and comments on your work in order to revise and improve your communication skills.